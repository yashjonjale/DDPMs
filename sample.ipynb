{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m, input_dim)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs726env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs726env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m, in \u001b[0;36mResidualMLPUnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m encoded, skip_connections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Decode using the skip connections to recover the original dimension\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_connections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/cs726env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs726env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, skip_connections)\u001b[0m\n\u001b[1;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(up(x))\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# Combine with corresponding skip connection (concatenation or addition)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Here we use element-wise addition assuming matching dimensions.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m  \n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block with two fully connected layers and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if hidden_features is None:\n",
    "            hidden_features = in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += residual  # Residual connection\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module that downsamples the input vector using linear layers followed by Residual Blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        layers = []\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        last_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            # Downsample the vector dimension using a linear layer\n",
    "            self.down_layers.append(nn.Linear(last_dim, h_dim))\n",
    "            layers.append(ResidualBlock(h_dim))\n",
    "            last_dim = h_dim\n",
    "        self.res_blocks = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down, block in zip(self.down_layers, self.res_blocks):\n",
    "            x = F.relu(down(x))\n",
    "            x = block(x)\n",
    "            skip_connections.append(x)\n",
    "        return x, skip_connections\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module that upsamples the encoded vector using linear layers and integrates skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, hidden_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        reversed_dims = list(reversed(hidden_dims))\n",
    "        last_dim = reversed_dims[0]\n",
    "        for h_dim in reversed_dims[1:]:\n",
    "            self.up_layers.append(nn.Linear(last_dim, h_dim))\n",
    "            self.res_blocks.append(ResidualBlock(h_dim))\n",
    "            last_dim = h_dim\n",
    "        self.final_layer = nn.Linear(last_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, skip_connections):\n",
    "        # Reverse the skip connections to match the decoder order\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        for up, block, skip in zip(self.up_layers, self.res_blocks, skip_connections[:-1]):\n",
    "            x = F.relu(up(x))\n",
    "            # Combine with corresponding skip connection (concatenation or addition)\n",
    "            # Here we use element-wise addition assuming matching dimensions.\n",
    "            x = x + skip  \n",
    "            x = block(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n",
    "\n",
    "class ResidualMLPUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Residual MLP UNet for fixed-size vector data.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(ResidualMLPUnet, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dims)\n",
    "        self.decoder = Decoder(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the input vector and store skip connections\n",
    "        encoded, skip_connections = self.encoder(x)\n",
    "        # Decode using the skip connections to recover the original dimension\n",
    "        out = self.decoder(encoded, skip_connections)\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Define input vector dimension and hidden dimensions for the encoder/decoder\n",
    "    input_dim = 128\n",
    "    hidden_dims = [256, 512, 256]  # Example configuration: encoder will go from 128->256->512->256\n",
    "\n",
    "    # Create model instance\n",
    "    model = ResidualMLPUnet(input_dim=input_dim, hidden_dims=hidden_dims)\n",
    "    \n",
    "    # Generate a random input vector batch of size (batch_size, input_dim)\n",
    "    x = torch.randn(16, input_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    print(\"Input shape:\", x.shape)\n",
    "    print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic residual block with two fully connected layers and a skip connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        if hidden_features is None:\n",
    "            hidden_features = in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.activation(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        out += residual  # Residual connection\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simplified Residual MLP UNet for fixed-size vector data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, bottleneck_dim, down_up_factor=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): The size of the input vector.\n",
    "            bottleneck_dim (int): The size of the bottleneck vector.\n",
    "            down_up_factor (int): The factor by which dimensions are divided when down-sampling\n",
    "                                  and multiplied when up-sampling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1) Generate down-sampling dimensions from input_dim -> bottleneck_dim\n",
    "        self.down_dims = self._compute_dims(input_dim, bottleneck_dim, down_up_factor)\n",
    "\n",
    "        # 2) Generate up-sampling dimensions by reversing the down dims\n",
    "        #    e.g., if down_dims = [input_dim, ..., bottleneck_dim]\n",
    "        #    then up_dims = [bottleneck_dim, ..., input_dim]\n",
    "        self.up_dims = list(reversed(self.down_dims))\n",
    "\n",
    "        # 3) Create down-sampling (encoder) layers and associated residual blocks\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        self.down_resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.down_dims) - 1):\n",
    "            in_dim = self.down_dims[i]\n",
    "            out_dim = self.down_dims[i + 1]\n",
    "            self.down_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            self.down_resblocks.append(ResidualBlock(out_dim))\n",
    "\n",
    "        # 4) Create up-sampling (decoder) layers and associated residual blocks\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.up_resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.up_dims) - 1):\n",
    "            in_dim = self.up_dims[i]\n",
    "            out_dim = self.up_dims[i + 1]\n",
    "            self.up_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            self.up_resblocks.append(ResidualBlock(out_dim))\n",
    "\n",
    "    def _compute_dims(self, start_dim, bottleneck_dim, factor):\n",
    "        \"\"\"\n",
    "        Compute the list of dimensions from start_dim down to bottleneck_dim\n",
    "        by dividing by 'factor' at each step (rounding up), ensuring it does\n",
    "        not go below bottleneck_dim.\n",
    "        \"\"\"\n",
    "        dims = [start_dim]\n",
    "        while dims[-1] > bottleneck_dim:\n",
    "            # Divide by factor and round up\n",
    "            next_dim = math.ceil(dims[-1] / factor)\n",
    "            if next_dim < bottleneck_dim:\n",
    "                next_dim = bottleneck_dim\n",
    "            dims.append(next_dim)\n",
    "        return dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------------------\n",
    "        # Down-sampling path\n",
    "        # ---------------------\n",
    "        skip_connections = []\n",
    "        for linear_layer, resblock in zip(self.down_layers, self.down_resblocks):\n",
    "            x = F.relu(linear_layer(x))\n",
    "            x = resblock(x)\n",
    "            # Save each \"down\" output for skip connection\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        # ---------------------\n",
    "        # Up-sampling path\n",
    "        # ---------------------\n",
    "        # Note: skip_connections is from smallest to largest in the forward pass,\n",
    "        # but we need them in reverse order for the up path.\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # We traverse up_layers normally, but each time we add the corresponding\n",
    "        # skip from the down path (with matching dimension).\n",
    "        for i, (linear_layer, resblock) in enumerate(zip(self.up_layers, self.up_resblocks)):\n",
    "            x = F.relu(linear_layer(x))\n",
    "            # Add skip connection (element-wise addition)\n",
    "            x = x + skip_connections[i + 1]  # +1 to skip the bottleneck skip\n",
    "            x = resblock(x)\n",
    "\n",
    "        # The final output of x should match input_dim in size since\n",
    "        # self.up_dims[-1] is the original input dimension.\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualMLPUnet(input_dim=128, bottleneck_dim=31, down_up_factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 86, 58, 39, 31]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs726env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
